---
title: "Practical Machine Learning - Prediction Course Project"
author: "Kevin E. D'Elia"
date: "September 10th, 2016"
output: 
  html_document: 
    highlight: espresso
    keep_md: yes
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
This paper summarizes the knowledge gained during the Practical Machine Learning module of the Coursera Data Science Specialization.  It attempts to predict

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

## Data

The training data for this project are available [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available 
[here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The data for this project come from this [source](http://groupware.les.inf.puc-rio.br/har). If you use their data for any purpose, please cite them in your work.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

[Read more](http://groupware.les.inf.puc-rio.br/har#ixzz4Jrvxp2iu)

This paper makes extensive use of the **caret** package.  More information about this package can be found [here](http://topepo.github.io/caret/index.html)

# Creating Training and Testing Datasets
## Load the Data

Before the data is loaded into R, it was viewed using a spreadsheet program.
Examination of the data reveals columns with spurious data, such as divide-by-zero indicators.  These were identified and specified to the _na.strings_ parameter of the **read.csv** method.

```{r load_data}
trainingData <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testingData <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```

## Load the necessary packages

The caret package will be needed for the data tidying work as well as model building, etc., so load it now.
```{r load_packages}
library(caret)
```
# Data Tidying
The initial data prep puts the datasets in a good state for data tidying.
```{r data_tidy_1}
dim(trainingData)
```
Clearly there are a lot (`r ncol(trainingData)`) of variables/predictors to choose from.  But not all of them might be suitable.  The task here is to eliminate/reduce the number of predictors to a reasonable working set.  The first step in that process is to look at the data.  Judging from the columns names seen in the spreadsheet (and, in the absence of a data dictionary), the first 7 columns are likely candidates for exclusion:
```{r data_tidy_2}
head(testingData[, 1:7], n = 3)
tail(testingData[, 1:7], n = 3)
```
These values are clearly not useful as predictor variables, so they will be removed from both the training and testing datasets.
```{r data_tidy_3}
trainingData <- trainingData[, 8:ncol(trainingData)]
testingData <- testingData[, 8:ncol(testingData)]
```

The data contains many columns with a large number of **NA** values.  Columns with 50% or greater of NA values will contribute nothing to the model.  These are removed with the following line of code:

```{r data_tidy_4}
trainingData <- trainingData[, colSums(is.na(trainingData)) < (nrow(trainingData) * 0.5)]
testingData <- testingData[, colSums(is.na(testingData)) < (nrow(testingData) * 0.5)]
```

The last step in the data tidying process is to check for any columns where the variance is near or equal to zero, meaning that these variables can potentially cause problems for modeling.
```{r data_tidy_5}
nearZeroVar(trainingData)
nearZeroVar(testingData)
```

The result, **integer(0)**, means that no columns have a near-zero variance (i.e., zeroVar and nzv display FALSE for all predictors when using the _saveMetrics_ parameter set to **TRUE**) and thus all the columns we have in the dataset can be considered adequate predictors.
The data tidying portion of the work is done; on to model building.

# Model Evaluation and Selection

## Reproducibility of work
In order to ensure consistent results across subsequent runs of the code, a seed value is set so that the same values will be generated for all the model training and fitting executions.
```{r set_seed}
set.seed(9999)
```

## Create Training and Testing datasets
In order to train the model and preserve the testing dataset, the original training data is split into two sub-datasets: training and testing.  It's a bit confusing but the purpose is to allow a higher degree of accuracy (i.e., low out-of-sample errors) when making predictions on the pristine testing dataset based on a refined model fit.  Basically, the procedure is:

1. split original training set into sub-training/test sets
2. build the model on sub-training set
3. evaluate the model on sub-test set
4. repeat and average estimated errors

This is accomplished using the _createDataPartition_ method in the **caret** package:

```{r train_and_test}
INDEX <- createDataPartition(y = trainingData$classe, p = 0.6, list = FALSE)
# Create a sub-training set using 60% of the original training data
trainingSet <- trainingData[INDEX,]
# Create a sub-testing set using 40% of the original training data
testingSet <- trainingData[-INDEX,]
```



Next, a trainControl instance is created, using 3-fold cross validation.

So the idea here is we break our data set up into k equal size data sets.
we would build a prediction model on k-1 folds and apply the model to the kth fold. So, dividing our dataset into 3 folds, the prediction model would be built on the first two folds, applied to the third fold and so on.
this training data. And we would apply it to this test data. Then we would build a training our model
• break training set into K subsets
• build the model/predictor on the remaining training data in each subset and applied to the test subset
• rebuild the data K times with the training and test subsets and average the findings
• considerations
        – larger k = less bias, more variance
        – smaller k = more bias, less variance
```{r training_data}
trainingControl <- trainControl(method = "cv", number = 3)
```
Once the trainingControl has been created, it is passed to the **train** function, which applies the desired machine learning algorithm to construct a model from the subset of the training data.
Three different tree-based algorithms will be evaluated:  

1. Decision Trees (rpart)
2. Gradient Boosting Trees (gbm)
3. Random Forest Decision Trees (rf)

```{r define_function, echo=FALSE}
trainer <- function(x) {
  train(classe ~ ., data = trainingSet, method = x, trControl = trainingControl)
}
```

```{r model_training, message=FALSE}
cartModel <- trainer("rpart")
gbmModel <- trainer("gbm")
rfModel <- trainer("rf")

#cartModel <- train(classe ~ ., data = trainingSet, method = "rpart", trControl = trainingControl)
#gbmModel <- train(classe ~ ., data = trainingSet, method = "gbm", trControl = trainingControl)
#rfModel <- train(classe ~ ., data = trainingSet, method = "rf", trControl = trainingControl)

#rfModel <- train(classe ~ ., data = trainingSet, method = "rf", trControl = trainingControl)
#rfModel$finalModel
```

# Model Retraining

# Prediction using Training and Test Sets
```{r prediction}
prediction <- predict(rfModel, newdata = trainingSet, type="raw")
confusionMatrix(prediction, trainingSet$classe)
prediction <- predict(rfModel, newdata = testingSet, type="raw")
confusionMatrix(prediction, testingSet$classe)
```

# Predictions Using Test Data
```{r predictions}
prediction <- predict(rfModel, testingData, type = "raw")
for (i in 1:length(prediction)) 
        print(paste0("Prediction ", i, " = ", prediction[i]))
```

```{r echo=FALSE}
#for (i in seq(20)) { 
#        fileName <- paste("Prediction", i, ".txt"); 
#        write.table(prediction[i], 
#                    file = fileName, 
#                    quote = FALSE, 
#                    row.names = FALSE, 
#                    col.names = FALSE)
#        }
```